import { dirname, extname, format, isAbsolute, join, parse, posix, relative, resolve, win32 } from "path";
import { existsSync, mkdirSync, readFileSync, statSync, writeFileSync } from "fs";
import { fileURLToPath } from "url";
import { decode } from "@jridgewell/sourcemap-codec";
import remapping from "@ampproject/remapping";
import { brotliCompressSync, gzipSync } from "zlib";

//#region ../load-source-map/dist/index.mjs
/**
* Strip any JSON XSSI avoidance prefix from the string (as documented in the source maps specification),
* and parses the string as JSON.
*
* https://github.com/mozilla/source-map/blob/3cb92cc3b73bfab27c146bae4ef2bc09dbb4e5ed/lib/util.js#L162-L164
*/
function parseSourceMapInput(str) {
	return JSON.parse(str.replace(/^\)]}'[^\n]*\n/, ""));
}
/**
sourceMappingURL=data:application/json;charset=utf-8;base64,data
sourceMappingURL=data:application/json;base64,data
sourceMappingURL=data:application/json;uri,data
sourceMappingURL=map-file-comment.css.map
sourceMappingURL=map-file-comment.css.map?query=value
*/
const sourceMappingRegExp = /[@#]\s*sourceMappingURL=(\S+)\b/g;
/**
* Checks if the given path is a file.
*/
function isFile(path) {
	try {
		return statSync(path).isFile();
	} catch {
		return false;
	}
}
function loadCodeAndMap(codePath, sourcesPathNormalizer) {
	if (!isFile(codePath)) return null;
	const code = readFileSync(codePath, "utf-8");
	const extractedComment = code.includes("sourceMappingURL") && Array.from(code.matchAll(sourceMappingRegExp)).at(-1);
	if (!extractedComment || !extractedComment.length) return { code };
	const maybeMap = loadMap(codePath, extractedComment[1]);
	if (!maybeMap) return { code };
	const { map, mapPath } = maybeMap;
	const mapDir = dirname(mapPath);
	sourcesPathNormalizer ??= (path) => isAbsolute(path) ? path : resolve(mapDir, map.sourceRoot ?? ".", path);
	map.sources = normalizeSourcesPaths(map, sourcesPathNormalizer);
	map.sourcesContent = loadMissingSourcesContent(map);
	delete map.sourceRoot;
	return {
		code,
		map
	};
}
function loadMap(codePath, sourceMappingURL) {
	if (sourceMappingURL.startsWith("data:")) {
		const map = parseDataUrl(sourceMappingURL);
		return {
			map: parseSourceMapInput(map),
			mapPath: codePath
		};
	}
	const sourceMapFilename = new URL(sourceMappingURL, "file://").pathname;
	const mapPath = join(dirname(codePath), sourceMapFilename);
	if (!existsSync(mapPath)) return null;
	return {
		map: parseSourceMapInput(readFileSync(mapPath, "utf-8")),
		mapPath
	};
}
function parseDataUrl(url) {
	const [prefix, payload] = url.split(",");
	const encoding = prefix.split(";").at(-1);
	switch (encoding) {
		case "base64": return Buffer.from(payload, "base64").toString();
		case "uri": return decodeURIComponent(payload);
		default: throw new Error("Unsupported source map encoding: " + encoding);
	}
}
/**
* Normalize the paths of the sources in the source map to be absolute paths.
*/
function normalizeSourcesPaths(map, sourcesPathNormalizer) {
	return map.sources.map((source) => source ? sourcesPathNormalizer(source) : null);
}
/**
* Loop through the sources and try to load missing `sourcesContent` from the file system.
*/
function loadMissingSourcesContent(map) {
	return map.sources.map((source, index) => {
		if (map.sourcesContent?.[index]) return map.sourcesContent[index];
		if (source && existsSync(source)) return readFileSync(source, "utf-8");
		return null;
	});
}

//#endregion
//#region src/utils.ts
const esmRegex = /\.m[tj]sx?$/;
const cjsRegex = /\.c[tj]sx?$/;
const jsRegexp = /\.[cm]?[tj]s[x]?$/;
function normalizeOptions(options) {
	const format$1 = options?.format || options?.filename?.split(".").at(-1) || "html";
	const defaultOptions = {
		enabled: true,
		format: format$1,
		filename: "sonda-report." + format$1,
		open: true,
		detailed: false,
		sources: false,
		gzip: false,
		brotli: false,
		sourcesPathNormalizer: null
	};
	const normalizedOptions = Object.assign({}, defaultOptions, options);
	normalizedOptions.filename = normalizeOutputPath(normalizedOptions);
	return normalizedOptions;
}
function normalizePath(pathToNormalize) {
	const normalized = pathToNormalize.replace(/^\0/, "");
	const relativized = relative(process.cwd(), normalized);
	return relativized.replaceAll(win32.sep, posix.sep);
}
function normalizeOutputPath(options) {
	let path = options.filename;
	const expectedExtension = "." + options.format;
	if (!isAbsolute(path)) path = join(process.cwd(), path);
	if (expectedExtension !== extname(path)) {
		console.warn("\x1B[0;33m" + `Sonda: The file extension specified in the 'filename' does not match the 'format' option. ` + `The extension will be changed to '${expectedExtension}'.`);
		path = format({
			...parse(path),
			base: "",
			ext: expectedExtension
		});
	}
	return path;
}

//#endregion
//#region src/sourcemap/map.ts
function mapSourceMap(map, dirPath, inputs) {
	const alreadyRemapped = new Set();
	const remapped = remapping(map, (file, ctx) => {
		if (alreadyRemapped.has(file)) return;
		alreadyRemapped.add(file);
		const codeMap = addSourcesToInputs(resolve(dirPath, file), inputs);
		if (!codeMap) return;
		ctx.content ??= codeMap.code;
		return codeMap.map;
	}, { decodedMappings: true });
	return remapped;
}
function addSourcesToInputs(path, inputs) {
	const codeMap = loadCodeAndMap(path);
	if (!codeMap) return null;
	const parentPath = normalizePath(path);
	const format$1 = inputs[parentPath]?.format ?? "unknown";
	codeMap.map?.sources.filter((source) => source !== null).forEach((source, index) => {
		const normalizedPath = normalizePath(source);
		if (parentPath === normalizedPath) return;
		inputs[normalizedPath] = {
			bytes: Buffer.byteLength(codeMap.map.sourcesContent?.[index] ?? ""),
			format: format$1,
			imports: [],
			belongsTo: parentPath
		};
	});
	return codeMap;
}

//#endregion
//#region src/sourcemap/bytes.ts
const UNASSIGNED = "[unassigned]";
function getBytesPerSource(code, map, assetSizes, options) {
	const contributions = getContributions(map.sources);
	const codeLines = code.split(/(?<=\r?\n)/);
	for (let lineIndex = 0; lineIndex < codeLines.length; lineIndex++) {
		const lineCode = codeLines[lineIndex];
		const mappings = map.mappings[lineIndex] || [];
		let currentColumn = 0;
		for (let i = 0; i <= mappings.length; i++) {
			const mapping = mappings[i];
			const startColumn = mapping?.[0] ?? lineCode.length;
			const endColumn = mappings[i + 1]?.[0] ?? lineCode.length;
			if (startColumn > currentColumn) contributions.set(UNASSIGNED, contributions.get(UNASSIGNED) + lineCode.slice(currentColumn, startColumn));
			if (mapping) {
				const sourceIndex = mapping?.[1];
				const codeSlice = lineCode.slice(startColumn, endColumn);
				const source = sourceIndex !== undefined && map.sources[sourceIndex] || UNASSIGNED;
				contributions.set(source, contributions.get(source) + codeSlice);
				currentColumn = endColumn;
			} else currentColumn = startColumn;
		}
	}
	const sourceSizes = new Map();
	const contributionsSum = {
		uncompressed: 0,
		gzip: 0,
		brotli: 0
	};
	for (const [source, codeSegment] of contributions) {
		const sizes = getSizes(codeSegment, options);
		contributionsSum.uncompressed += sizes.uncompressed;
		contributionsSum.gzip += sizes.gzip;
		contributionsSum.brotli += sizes.brotli;
		sourceSizes.set(source, sizes);
	}
	return adjustSizes(sourceSizes, assetSizes, contributionsSum, options);
}
function getSizes(code, options) {
	return {
		uncompressed: Buffer.byteLength(code),
		gzip: options.gzip ? gzipSync(code).length : 0,
		brotli: options.brotli ? brotliCompressSync(code).length : 0
	};
}
function getContributions(sources) {
	const contributions = new Map();
	sources.filter((source) => source !== null).forEach((source) => contributions.set(source, ""));
	contributions.set(UNASSIGNED, "");
	return contributions;
}
/**
* Compression efficiency improves with the size of the file.
*
* However, what we have is the compressed size of the entire bundle (`actual`),
* the sum of all files compressed individually (`sum`) and the compressed
* size of a given file (`content`). The last value is essentially a “worst-case”
* scenario, and the actual size of the file in the bundle is likely to be smaller.
*
* We use this information to estimate the actual size of the file in the bundle
* after compression.
*/
function adjustSizes(sources, asset, sums, options) {
	const gzipDelta = options.gzip ? asset.gzip / sums.gzip : 0;
	const brotliDelta = options.brotli ? asset.brotli / sums.brotli : 0;
	for (const [source, sizes] of sources) sources.set(source, {
		uncompressed: sizes.uncompressed,
		gzip: options.gzip ? Math.round(sizes.gzip * gzipDelta) : 0,
		brotli: options.brotli ? Math.round(sizes.brotli * brotliDelta) : 0
	});
	return sources;
}

//#endregion
//#region src/report/formats.ts
function generateJsonReport(assets, inputs, options) {
	const acceptedExtensions = [
		".js",
		".mjs",
		".cjs",
		".css"
	];
	const outputs = assets.filter((asset) => acceptedExtensions.includes(extname(asset))).reduce((carry, asset) => {
		const data = processAsset(asset, inputs, options);
		if (data) carry[normalizePath(asset)] = data;
		return carry;
	}, {});
	return {
		inputs: sortObjectKeys(inputs),
		outputs: sortObjectKeys(outputs)
	};
}
function generateHtmlReport(assets, inputs, options) {
	const json = generateJsonReport(assets, inputs, options);
	const __dirname = dirname(fileURLToPath(import.meta.url));
	const template = readFileSync(resolve(__dirname, "./index.html"), "utf-8");
	return template.replace("__REPORT_DATA__", encodeURIComponent(JSON.stringify(json)));
}
function processAsset(asset, inputs, options) {
	const maybeCodeMap = loadCodeAndMap(asset, options.sourcesPathNormalizer);
	if (!hasCodeAndMap(maybeCodeMap)) return;
	const { code, map } = maybeCodeMap;
	const mapped = options.detailed ? mapSourceMap(map, dirname(asset), inputs) : {
		...map,
		mappings: decode(map.mappings)
	};
	mapped.sources = mapped.sources.map((source) => source && normalizePath(source));
	const assetSizes = getSizes(code, options);
	const bytes = getBytesPerSource(code, mapped, assetSizes, options);
	const outputInputs = Array.from(bytes).reduce((carry, [source, sizes]) => {
		carry[normalizePath(source)] = sizes;
		return carry;
	}, {});
	return {
		...assetSizes,
		inputs: sortObjectKeys(outputInputs),
		map: options.sources ? {
			version: 3,
			names: [],
			mappings: mapped.mappings,
			sources: mapped.sources,
			sourcesContent: mapped.sourcesContent
		} : undefined
	};
}
function hasCodeAndMap(result) {
	return Boolean(result && result.code && result.map);
}
function sortObjectKeys(object) {
	return Object.keys(object).sort().reduce((carry, key) => {
		carry[key] = object[key];
		return carry;
	}, {});
}

//#endregion
//#region src/report/generate.ts
async function generateReportFromAssets(assets, inputs, pluginOptions) {
	const options = normalizeOptions(pluginOptions);
	const handler = options.format === "html" ? saveHtml : saveJson;
	const report = handler(assets, inputs, options);
	const outputDirectory = dirname(options.filename);
	if (!existsSync(outputDirectory)) mkdirSync(outputDirectory, { recursive: true });
	writeFileSync(options.filename, report);
	if (!options.open) return;
	/**
	* `open` is ESM-only package, so we need to import it
	* dynamically to make it work in CommonJS environment.
	*/
	const { default: open } = await import("open");
	open(options.filename);
}
function saveHtml(assets, inputs, options) {
	return generateHtmlReport(assets, inputs, options);
}
function saveJson(assets, inputs, options) {
	const report = generateJsonReport(assets, inputs, options);
	return JSON.stringify(report, null, 2);
}

//#endregion
export { addSourcesToInputs, cjsRegex, esmRegex, generateReportFromAssets, jsRegexp, normalizePath };
//# sourceMappingURL=src.mjs.map